"""
ê³ ê¸‰ ì „ì²˜ë¦¬ ë° NLP ëª¨ë“ˆ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import yaml
import json
from src.preprocessing.advanced_preprocessor import AdvancedPreprocessor
from src.nlp.advanced_intent_classifier import AdvancedIntentClassifier


def compare_preprocessing():
    """ì „ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ"""
    print("=" * 80)
    print("ì „ì²˜ë¦¬ ëª¨ë“ˆ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    with open('config/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    preprocessor = AdvancedPreprocessor(config)
    
    # ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡ ì²˜ë¦¬ (ë³‘í•© ë¹„í™œì„±í™”)
    print("\n[í…ŒìŠ¤íŠ¸ 1] ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡ - ë³‘í•© ë¹„í™œì„±í™”")
    print("-" * 80)
    utterances = preprocessor.process(
        input_path='data/input/natural_meeting_transcript.txt',
        output_path='data/output/utterances_natural_no_merge.json',
        enable_merge=False
    )
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\nì •ì œ íš¨ê³¼:")
    for i, utt in enumerate(utterances[:3]):
        print(f"\n{i+1}. [{utt.speaker}]")
        print(f"   ì›ë³¸: {utt.original_text}")
        print(f"   ì •ì œ: {utt.text}")
    
    # ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡ ì²˜ë¦¬ (ë³‘í•© í™œì„±í™”)
    print("\n\n[í…ŒìŠ¤íŠ¸ 2] ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡ - ë³‘í•© í™œì„±í™”")
    print("-" * 80)
    utterances_merged = preprocessor.process(
        input_path='data/input/natural_meeting_transcript.txt',
        output_path='data/output/utterances_natural_merged.json',
        enable_merge=True
    )
    
    print(f"\në³‘í•© íš¨ê³¼:")
    print(f"  - ë³‘í•© ì „: {len(utterances)}ê°œ ë°œí™”")
    print(f"  - ë³‘í•© í›„: {len(utterances_merged)}ê°œ ë°œí™”")
    
    # ë³‘í•©ëœ ë°œí™” í™•ì¸
    merged_count = sum(1 for utt in utterances_merged if len(utt.merged_from) > 0)
    print(f"  - ë³‘í•©ëœ ë°œí™”: {merged_count}ê°œ")


def compare_nlp():
    """NLP ëª¨ë“ˆ ë¹„êµ"""
    print("\n\n" + "=" * 80)
    print("NLP ëª¨ë“ˆ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    with open('config/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    classifier = AdvancedIntentClassifier(config)
    
    # ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡ ë¶„ì„
    print("\n[ë¶„ì„] ìì—°ìŠ¤ëŸ¬ìš´ íšŒì˜ë¡")
    print("-" * 80)
    intents = classifier.process(
        utterances_path='data/output/utterances_natural_no_merge.json',
        output_path='data/output/intents_natural_advanced.json'
    )
    
    # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
    print("\n\nìƒì„¸ ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 5ê°œ):")
    print("-" * 80)
    for i, intent in enumerate(intents[:5]):
        print(f"\n{i+1}. {intent.intent_type} (ì‹ ë¢°ë„: {intent.confidence:.2f})")
        if intent.sub_intent:
            print(f"   ì„¸ë¶€ ì˜ë„: {intent.sub_intent}")
        print(f"   ê°ì •: {intent.sentiment}")
        print(f"   í‚¤ì›Œë“œ: {', '.join(intent.keywords[:5])}")


def show_statistics():
    """í†µê³„ ë¹„êµ"""
    print("\n\n" + "=" * 80)
    print("ì „ì²´ í†µê³„ ë¹„êµ")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    with open('data/output/utterances_natural_no_merge.json', 'r', encoding='utf-8') as f:
        utt_data = json.load(f)
    
    with open('data/output/intents_natural_advanced.json', 'r', encoding='utf-8') as f:
        intent_data = json.load(f)
    
    print("\n[ì „ì²˜ë¦¬ í†µê³„]")
    print(f"  ì´ ë°œí™” ìˆ˜: {utt_data['metadata']['total_utterances']}")
    print(f"  í™”ì ìˆ˜: {len(utt_data['metadata']['speakers'])}")
    print(f"  í™”ì ëª©ë¡: {', '.join(utt_data['metadata']['speakers'])}")
    
    print("\n  í™”ìë³„ ë°œí™” í†µê³„:")
    for speaker, stats in utt_data['metadata']['speaker_statistics'].items():
        print(f"    - {speaker}: {stats['count']}ê°œ, í‰ê·  {stats['avg_length']:.1f}ì")
    
    print("\n[NLP ë¶„ì„ í†µê³„]")
    dist = intent_data['metadata']['distribution']
    
    print(f"  ì˜ë„ ë¶„í¬:")
    for intent_type, count in dist['intent_types'].items():
        percentage = (count / intent_data['metadata']['total_intents']) * 100
        print(f"    - {intent_type}: {count}ê°œ ({percentage:.1f}%)")
    
    print(f"\n  ì„¸ë¶€ ì˜ë„ ë¶„í¬:")
    for sub_intent, count in dist['sub_intents'].items():
        print(f"    - {sub_intent}: {count}ê°œ")
    
    print(f"\n  ê°ì • ë¶„í¬:")
    for sentiment, count in dist['sentiments'].items():
        percentage = (count / intent_data['metadata']['total_intents']) * 100
        print(f"    - {sentiment}: {count}ê°œ ({percentage:.1f}%)")
    
    print(f"\n  ìƒìœ„ í‚¤ì›Œë“œ (Top 10):")
    for kw_data in intent_data['metadata']['top_keywords'][:10]:
        print(f"    - {kw_data['keyword']}: {kw_data['count']}íšŒ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n")
    print("â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 20 + "ê³ ê¸‰ ì „ì²˜ë¦¬ & NLP ëª¨ë“ˆ í†µí•© í…ŒìŠ¤íŠ¸" + " " * 24 + "â•‘")
    print("â•š" + "=" * 78 + "â•")
    
    try:
        # ì „ì²˜ë¦¬ ë¹„êµ
        compare_preprocessing()
        
        # NLP ë¹„êµ
        compare_nlp()
        
        # í†µê³„ ë¹„êµ
        show_statistics()
        
        print("\n\n" + "=" * 80)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 80)
        
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("  - data/output/utterances_natural_no_merge.json")
        print("  - data/output/utterances_natural_merged.json")
        print("  - data/output/intents_natural_advanced.json")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
